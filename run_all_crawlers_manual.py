"""
æ‰‹åŠ¨æ‰§è¡Œæ‰€æœ‰çˆ¬è™«ä»»åŠ¡
åŒ…æ‹¬ï¼šæ™ºæ±‡æœŸè®¯ã€æ–¹æœŸçœ‹ç›˜ã€äº¤æ˜“å¯æŸ¥(è“å›¾+å¸­ä½)ã€Openvlab
"""
import asyncio
import logging
import sys
from pathlib import Path
from datetime import date, datetime
import json

sys.path.append(str(Path(__file__).parent))

from app.crawlers.zhihui_spider import ZhihuiQixunSpider
from app.crawlers.fangqi_spider import FangqiSpider
from app.crawlers.jiaoyikecha_spider import JiaoyiKechaSpider
from app.crawlers.openvlab_spider import OpenvlabSpider
from app.models.database import SessionLocal
from app.models.models import (
    DailyBlueprint, FundamentalReport,
    InstitutionalPosition, OptionFlow
)

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


async def crawl_zhihui():
    """çˆ¬å–æ™ºæ±‡æœŸè®¯æ•°æ®"""
    print("\n" + "=" * 60)
    print("ã€1/4ã€‘æ™ºæ±‡æœŸè®¯ - å¤šç©ºå…¨æ™¯")
    print("=" * 60)

    try:
        spider = ZhihuiQixunSpider()
        data = spider.fetch_full_view()

        if data:
            db = SessionLocal()
            try:
                for item in data:
                    report = FundamentalReport(
                        comm_code=item.get('variety_code', ''),
                        source='hzzhqx',
                        report_type='fullview',
                        sentiment=item.get('main_sentiment', 'neutral'),
                        content_summary=f"çœ‹å¤š {item.get('excessive_ratio', 0)}%, çœ‹ç©º {item.get('empty_ratio', 0)}%",
                        publish_time=datetime.now()
                    )
                    db.add(report)
                db.commit()
                print(f"âœ… æ™ºæ±‡æœŸè®¯: æˆåŠŸä¿å­˜ {len(data)} æ¡æ•°æ®")
            finally:
                db.close()
        else:
            print("âŒ æ™ºæ±‡æœŸè®¯: æœªè·å–åˆ°æ•°æ®")

    except Exception as e:
        print(f"âŒ æ™ºæ±‡æœŸè®¯çˆ¬å–å¤±è´¥: {e}")


async def crawl_fangqi():
    """çˆ¬å–æ–¹æœŸçœ‹ç›˜æ•°æ®"""
    print("\n" + "=" * 60)
    print("ã€2/4ã€‘æ–¹æœŸçœ‹ç›˜ - å¤œç›˜æç¤º")
    print("=" * 60)

    try:
        spider = FangqiSpider()
        night_data = await spider.fetch_night_data()

        if night_data:
            varieties = spider.parse_variety_list(night_data)
            db = SessionLocal()
            try:
                for item in varieties:
                    sentiment = 'bull' if item['direction'] == 'å¤š' else 'bear'
                    report = FundamentalReport(
                        comm_code=item.get('variety_code', ''),
                        source='founderfu',
                        report_type='night',
                        sentiment=sentiment,
                        content_summary=f"{item['smallbreeds']} - é£é™©å€¼:{item['rating']}",
                        publish_time=datetime.now()
                    )
                    db.add(report)
                db.commit()
                print(f"âœ… æ–¹æœŸçœ‹ç›˜: æˆåŠŸä¿å­˜ {len(varieties)} æ¡æ•°æ®")
            finally:
                db.close()
        else:
            print("âŒ æ–¹æœŸçœ‹ç›˜: æœªè·å–åˆ°æ•°æ®")

    except Exception as e:
        print(f"âŒ æ–¹æœŸçœ‹ç›˜çˆ¬å–å¤±è´¥: {e}")


async def crawl_jiaoyikecha():
    """çˆ¬å–äº¤æ˜“å¯æŸ¥æ•°æ® - è“å›¾ + å¸­ä½"""
    print("\n" + "=" * 60)
    print("ã€3/4ã€‘äº¤æ˜“å¯æŸ¥ - è“å›¾ + å¸­ä½æŒä»“")
    print("=" * 60)

    spider = JiaoyiKechaSpider()
    db = SessionLocal()

    try:
        print("åˆå§‹åŒ–æµè§ˆå™¨...")
        await spider.init_browser(headless=True)

        print("ç™»å½•äº¤æ˜“å¯æŸ¥...")
        if not await spider.login():
            print("âŒ ç™»å½•å¤±è´¥")
            return

        print("âœ… ç™»å½•æˆåŠŸ")

        # 1. è·å–äº¤æ˜“è“å›¾
        print("\nè·å–äº¤æ˜“è“å›¾...")
        blueprint = await spider.fetch_daily_blueprint()

        if blueprint:
            date_str = blueprint['date']
            record_date = date(int(date_str[:4]), int(date_str[4:6]), int(date_str[6:8]))

            strategies_json = json.dumps(blueprint.get('strategies', []), ensure_ascii=False)

            # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨
            existing = db.query(DailyBlueprint).filter(
                DailyBlueprint.record_date == record_date
            ).first()

            if existing:
                existing.image_url = blueprint['image_url']
                existing.local_path = blueprint['local_path']
                existing.parsed_strategies = strategies_json
            else:
                new_blueprint = DailyBlueprint(
                    image_url=blueprint['image_url'],
                    local_path=blueprint['local_path'],
                    parsed_strategies=strategies_json,
                    record_date=record_date
                )
                db.add(new_blueprint)

            db.commit()
            print(f"âœ… äº¤æ˜“è“å›¾: æˆåŠŸä¿å­˜ ({record_date})")
            print(f"   ç­–ç•¥æ•°: {len(blueprint.get('strategies', []))}")
        else:
            print("âŒ æœªè·å–åˆ°è“å›¾")

        # 2. è·å–å¸­ä½æŒä»“æ•°æ®
        print("\nè·å–å¸­ä½æŒä»“æ•°æ®...")
        varieties = ['rb', 'hc', 'i', 'j', 'jm', 'cu', 'al', 'zn', 'au', 'ag']

        saved_count = 0
        for variety_code in varieties:
            print(f"  æ­£åœ¨è·å– {variety_code.upper()} å¸­ä½æ•°æ®...")
            positions = await spider.fetch_variety_positions(variety_code)

            if positions:
                for pos in positions:
                    try:
                        net_pos = pos.get('net_position', '0')
                        if isinstance(net_pos, str):
                            net_pos = int(net_pos.replace(',', '').replace(' ', '') or '0')
                        change = pos.get('change', '0')
                        if isinstance(change, str):
                            change = int(change.replace(',', '').replace(' ', '') or '0')

                        position = InstitutionalPosition(
                            comm_code=variety_code.upper(),
                            broker_name=pos.get('broker', ''),
                            net_position=net_pos,
                            position_change=change,
                            record_date=date.today(),
                            created_at=datetime.now()
                        )
                        db.add(position)
                        saved_count += 1
                    except (ValueError, TypeError) as e:
                        logger.warning(f"è§£æ {variety_code} æŒä»“æ•°æ®å¤±è´¥: {e}")
                        continue

            await asyncio.sleep(2)  # å»¶è¿Ÿé¿å…è¯·æ±‚è¿‡å¿«

        db.commit()
        print(f"âœ… å¸­ä½æŒä»“: æˆåŠŸä¿å­˜ {saved_count} æ¡æ•°æ®")

    except Exception as e:
        print(f"âŒ äº¤æ˜“å¯æŸ¥çˆ¬å–å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        db.rollback()
    finally:
        await spider.close()
        db.close()


async def crawl_openvlab():
    """çˆ¬å–OpenvlabæœŸæƒæµæ•°æ®"""
    print("\n" + "=" * 60)
    print("ã€4/4ã€‘Openvlab - æœŸæƒèµ„é‡‘æµ")
    print("=" * 60)

    spider = OpenvlabSpider()

    try:
        print("åˆå§‹åŒ–æµè§ˆå™¨...")
        await spider.init_browser(headless=True)

        print("è·å–æœŸæƒæµæ•°æ®...")
        option_flow = await spider.fetch_option_flow_data()

        if option_flow:
            db = SessionLocal()
            try:
                for item in option_flow:
                    flow_record = OptionFlow(
                        comm_code=item.get('variety', ''),
                        contract_code=item.get('contract_code', ''),
                        net_flow=item.get('net_flow', 0),
                        volume=item.get('volume', 0),
                        change_ratio=item.get('change_ratio', 0),
                        record_time=datetime.now(),
                        created_at=datetime.now()
                    )
                    db.add(flow_record)

                db.commit()
                print(f"âœ… Openvlab: æˆåŠŸä¿å­˜ {len(option_flow)} æ¡æ•°æ®")
            finally:
                db.close()
        else:
            print("âŒ Openvlab: æœªè·å–åˆ°æ•°æ®")

    except Exception as e:
        print(f"âŒ Openvlabçˆ¬å–å¤±è´¥: {e}")
    finally:
        await spider.close()


async def main():
    """ä¸»å‡½æ•° - é¡ºåºæ‰§è¡Œæ‰€æœ‰çˆ¬è™«"""
    print("\n" + "ğŸš€" * 30)
    print("å¼€å§‹æ‰‹åŠ¨æ‰§è¡Œæ‰€æœ‰çˆ¬è™«ä»»åŠ¡")
    print(f"æ‰§è¡Œæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print("ğŸš€" * 30)

    start_time = datetime.now()

    # é¡ºåºæ‰§è¡Œæ‰€æœ‰çˆ¬è™«
    await crawl_zhihui()
    await crawl_fangqi()
    await crawl_jiaoyikecha()
    await crawl_openvlab()

    end_time = datetime.now()
    duration = (end_time - start_time).total_seconds()

    print("\n" + "=" * 60)
    print(f"âœ… æ‰€æœ‰çˆ¬è™«ä»»åŠ¡æ‰§è¡Œå®Œæˆ")
    print(f"   æ€»è€—æ—¶: {duration:.2f} ç§’")
    print("=" * 60)


if __name__ == "__main__":
    asyncio.run(main())
